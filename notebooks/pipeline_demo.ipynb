{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team-1 Data Synthesis Pipeline Demo\n",
    "\n",
    "This notebook demonstrates how to use the Team-1 Data Synthesis Pipeline to generate reasoning-focused Vision-Language datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import pipeline\n",
    "from team1_pipeline import DataSynthesisPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n",
      "which: no ccache in (/home/shu4/koa_scratch/DCVLR/.venv/bin:/mnt/lustre/koa/scratch/shu4/DCVLR/.venv/bin:/home/shu4/.vscode/cli/servers/Stable-cb1933bbc38d329b3595673a600fab5c7368f0a7/server/bin/remote-cli:/opt/apps/software/lang/Anaconda3/2024.02-1:/opt/apps/software/lang/Anaconda3/2024.02-1/sbin:/opt/apps/software/lang/Anaconda3/2024.02-1/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/misc/motd:/home/shu4/.local/bin:/home/shu4/bin:/opt/misc/motd/:/home/shu4/.local/bin:/home/shu4/bin:/opt/misc/motd/:/home/shu4/.local/bin:/home/shu4/bin)\n",
      "/home/shu4/koa_scratch/DCVLR/.venv/lib/python3.11/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/shu4/.paddlex/official_models/PP-LCNet_x1_0_doc_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/shu4/.paddlex/official_models/UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/shu4/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/shu4/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('en_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/shu4/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.\u001b[0m\n",
      "/home/shu4/koa_scratch/DCVLR/.venv/lib/python3.11/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[2025-11-18 01:12:53,689] [ WARNING] feature_extractor.py:43 - Could not load OCR model: No module named 'easyocr'\n",
      "/home/shu4/koa_scratch/DCVLR/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a618cd475647b98c3aac744efd30b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4f51efd5704b89ae8b9db1a20d3274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d253b532d0cb4a65839238d430e4e4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'images_dir': '../data/TextVQA',  # Using TextVQA dataset\n",
    "    'output_dir': '../output/demo',\n",
    "    'config_path': '../configs/default_config.yaml',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataSynthesisPipeline(\n",
    "    config_path=config['config_path'],\n",
    "    images_dir=config['images_dir'],\n",
    "    output_dir=config['output_dir'],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "print(f\"Pipeline initialized on {config['device']}\")\n",
    "print(f\"Using images from: {config['images_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Individual Pipeline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Filter Images\n",
    "print(\"Stage 1: Filtering images...\")\n",
    "filtered_images = pipeline.filter_stage(num_images=100)\n",
    "print(f\"Filtered to {len(filtered_images)} images\")\n",
    "\n",
    "# Display sample filtered images\n",
    "if filtered_images:\n",
    "    fig, axes = plt.subplots(1, min(3, len(filtered_images)), figsize=(12, 4))\n",
    "    for i, img_data in enumerate(filtered_images[:3]):\n",
    "        img = Image.open(img_data['path'])\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Image {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Bin Images\n",
    "print(\"Stage 2: Binning images...\")\n",
    "binned_images = pipeline.bin_stage(filtered_images, bins_ratio=(0.4, 0.4, 0.2))\n",
    "\n",
    "# Display bin distribution\n",
    "bin_counts = {k: len(v) for k, v in binned_images.items()}\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(bin_counts.keys(), bin_counts.values())\n",
    "plt.xlabel('Bin Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Image Distribution Across Bins')\n",
    "for i, (k, v) in enumerate(bin_counts.items()):\n",
    "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bin A (Text/Arithmetic): {bin_counts['A']} images\")\n",
    "print(f\"Bin B (Object/Spatial): {bin_counts['B']} images\")\n",
    "print(f\"Bin C (Commonsense/Attribute): {bin_counts['C']} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Generate Q/A Pairs\n",
    "print(\"Stage 3: Generating Q/A pairs...\")\n",
    "qa_dataset = pipeline.synthesis_stage(binned_images)\n",
    "print(f\"Generated {len(qa_dataset)} Q/A pairs\")\n",
    "\n",
    "# Display sample Q/A pairs\n",
    "if qa_dataset:\n",
    "    sample_qa = qa_dataset[0]\n",
    "    print(\"\\nSample Q/A pair:\")\n",
    "    print(f\"Image: {sample_qa['image']}\")\n",
    "    print(f\"Bin: {sample_qa['bin']}\")\n",
    "    print(f\"Question: {sample_qa.get('question', 'N/A')}\")\n",
    "    print(f\"Answer: {sample_qa.get('answer', 'N/A')}\")\n",
    "    print(f\"Reasoning: {sample_qa.get('reasoning', 'N/A')[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: Validate Dataset\n",
    "print(\"Stage 4: Validating dataset...\")\n",
    "validated_dataset = pipeline.validation_stage(qa_dataset)\n",
    "print(f\"Validated {len(validated_dataset)} Q/A pairs\")\n",
    "print(f\"Removed {len(qa_dataset) - len(validated_dataset)} invalid entries\")\n",
    "\n",
    "# Save validated dataset\n",
    "pipeline.save_dataset(validated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the dataset\n",
    "dataset_path = Path(config['output_dir']) / 'synthetic_qa_dataset.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(dataset_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total Q/A pairs: {len(df)}\")\n",
    "print(f\"\\nDistribution by bin:\")\n",
    "print(df['bin'].value_counts())\n",
    "\n",
    "# Question length distribution\n",
    "df['question_length'] = df['question'].str.split().str.len()\n",
    "df['answer_length'] = df['answer'].str.split().str.len()\n",
    "df['reasoning_length'] = df['reasoning'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(df['question_length'], bins=20)\n",
    "axes[0].set_xlabel('Question Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Question Length Distribution')\n",
    "\n",
    "axes[1].hist(df['answer_length'], bins=20)\n",
    "axes[1].set_xlabel('Answer Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Answer Length Distribution')\n",
    "\n",
    "axes[2].hist(df['reasoning_length'], bins=20)\n",
    "axes[2].set_xlabel('Reasoning Length (words)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Reasoning Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline end-to-end\n",
    "results = pipeline.run(\n",
    "    num_images=100,\n",
    "    bins_ratio=(0.4, 0.4, 0.2),\n",
    "    skip_benchmarking=True  # Skip benchmarking for demo\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPipeline Results:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample Q/A pairs from each bin\n",
    "for bin_type in ['A', 'B', 'C']:\n",
    "    bin_samples = df[df['bin'] == bin_type].head(2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Bin {bin_type} Samples:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for idx, row in bin_samples.iterrows():\n",
    "        print(f\"\\nQuestion: {row['question']}\")\n",
    "        print(f\"Answer: {row['answer']}\")\n",
    "        print(f\"Reasoning: {row['reasoning'][:150]}...\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
